from kafka import KafkaProducer
import json, time

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

print("ðŸš€ Sending streaming data to topic: input_topic")

i = 0
while True:
    msg = {"id": i, "message": f"Hello Kafka Stream {i}"}
    producer.send("input_topic", msg)
    print("âœ… Sent:", msg)
    i += 1
    time.sleep(2)  # send every 2 seconds



from pyspark.sql import SparkSession
from pyspark.sql.functions import expr

spark = SparkSession.builder.appName("KafkaStreamConsumer").getOrCreate()

df_stream = spark.readStream.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "input_topic") \
    .option("startingOffsets", "latest") \
    .load()

df_stream = df_stream.selectExpr("CAST(value AS STRING) as json_value")

# You can parse the JSON if needed, e.g. using from_json
df_transformed = df_stream.withColumn("processed_time", expr("current_timestamp()"))

query = df_transformed.writeStream \
    .format("console") \
    .option("truncate", "false") \
    .start()

query.awaitTermination()


from kafka import KafkaProducer
import json

producer = KafkaProducer(
    bootstrap_servers='localhost:9092',
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

data = [
    {"id": 1, "message": "Batch message 1"},
    {"id": 2, "message": "Batch message 2"},
    {"id": 3, "message": "Batch message 3"}
]

for record in data:
    producer.send("input_topic", record)
    print("âœ… Sent:", record)

producer.flush()
print("ðŸŽ¯ All batch data sent to Kafka topic: input_topic")


from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("KafkaBatchConsumer").getOrCreate()

df_batch = spark.read.format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "input_topic") \
    .option("startingOffsets", "earliest") \
    .option("endingOffsets", "latest") \
    .load()

df_batch = df_batch.selectExpr("CAST(value AS STRING) as message")

df_batch.show(truncate=False)
df_batch.write.mode("overwrite").parquet("/tmp/kafka_batch_output")

print("âœ… Kafka batch data written to /tmp/kafka_batch_output")
